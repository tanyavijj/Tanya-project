{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanyavijj/Tanya-project/blob/main/SRGAN_ERROR_Notebook_Code_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "411f8429",
      "metadata": {
        "id": "411f8429"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "45e4e25b",
      "metadata": {
        "id": "45e4e25b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7848087e-2e58-4e13-d748-6eaa92d5c8f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# STEP 0: INSTALL DEPENDENCIES\n",
        "%pip install -q torchvision scikit-image matplotlib opencv-python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b92e1b9e",
      "metadata": {
        "id": "b92e1b9e"
      },
      "outputs": [],
      "source": [
        "# STEP 1: DOWNLOAD + TRIM DATASET\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import urllib.request\n",
        "from zipfile import ZipFile\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "url = \"https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
        "zip_path = \"data/DIV2K_train_HR.zip\"\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "with ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"data/\")\n",
        "\n",
        "# Trim to first 100 images\n",
        "all_images = sorted(glob.glob(\"data/DIV2K_train_HR/*.png\"))\n",
        "for img in all_images[100:]:\n",
        "    os.remove(img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "098eaf23",
      "metadata": {
        "id": "098eaf23"
      },
      "outputs": [],
      "source": [
        "# STEP 2: CONFIG\n",
        "batch_size = 2   # Reduce to fit GPU memory\n",
        "crop_size = 64   # Smaller crop to reduce resolution and memory\n",
        "upscale_factor = 4\n",
        "num_epochs_pretrain = 2\n",
        "num_epochs_gan = 5\n",
        "lr = 1e-4\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "\n",
        "train_hr_path = 'data/DIV2K_train_HR'\n",
        "model_save_path = 'models'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "df532d85",
      "metadata": {
        "id": "df532d85"
      },
      "outputs": [],
      "source": [
        "# STEP 3: DATASET LOADER\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, image_dir, crop_size=64, upscale_factor=4):\n",
        "        self.image_filenames = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
        "        self.hr_crop_size = crop_size * upscale_factor\n",
        "        self.lr_size = crop_size\n",
        "        self.upscale_factor = upscale_factor\n",
        "\n",
        "        self.hr_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(self.hr_crop_size),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.lr_downscale = transforms.Resize(self.lr_size, interpolation=Image.BICUBIC)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hr = Image.open(self.image_filenames[index]).convert(\"RGB\")\n",
        "        hr = self.hr_transform(hr)\n",
        "        lr = self.lr_downscale(transforms.ToPILImage()(hr))\n",
        "        lr = transforms.ToTensor()(lr)\n",
        "        return lr, hr\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "\n",
        "train_dataset = SRDataset(train_hr_path, crop_size, upscale_factor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eb97351c",
      "metadata": {
        "id": "eb97351c"
      },
      "outputs": [],
      "source": [
        "# STEP 4: GENERATOR\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_res_blocks=16):\n",
        "        super().__init__()\n",
        "        self.input_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.res_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_res_blocks)])\n",
        "        self.mid_conv = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Conv2d(64, 256, kernel_size=3, padding=1),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(64, 256, kernel_size=3, padding=1),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.output_conv = nn.Conv2d(64, 3, kernel_size=9, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.input_conv(x)\n",
        "        x2 = self.res_blocks(x1)\n",
        "        x3 = self.mid_conv(x2)\n",
        "        x4 = self.upsample(x1 + x3)\n",
        "        return self.output_conv(x4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0d61bdb6",
      "metadata": {
        "id": "0d61bdb6"
      },
      "outputs": [],
      "source": [
        "# STEP 5: DISCRIMINATOR + VGG\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        def block(in_channels, out_channels, stride):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 3, stride, 1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            )\n",
        "        self.net = nn.Sequential(\n",
        "            block(3, 64, 1),\n",
        "            block(64, 64, 2),\n",
        "            block(64, 128, 1),\n",
        "            block(128, 128, 2),\n",
        "            block(128, 256, 1),\n",
        "            block(256, 256, 2),\n",
        "            block(256, 512, 1),\n",
        "            block(512, 512, 2),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class VGGContentLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = torchvision.models.vgg19(pretrained=True).features\n",
        "        self.feature_extractor = nn.Sequential(*list(vgg[:36])).eval()\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        return nn.functional.mse_loss(self.feature_extractor(sr), self.feature_extractor(hr))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5ed36a28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ed36a28",
        "outputId": "d845c696-c737-4a07-b191-4efa9dad639c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrain Epoch [1/2] Loss: 0.0232\n",
            "Pretrain Epoch [2/2] Loss: 0.0485\n"
          ]
        }
      ],
      "source": [
        "# STEP 6: PRETRAIN GENERATOR\n",
        "generator = Generator().cuda()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs_pretrain):\n",
        "    generator.train()\n",
        "    for lr_img, hr_img in train_loader:\n",
        "        lr_img, hr_img = lr_img.cuda(), hr_img.cuda()\n",
        "        sr_img = generator(lr_img)\n",
        "        loss = criterion(sr_img, hr_img)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Pretrain Epoch [{epoch+1}/{num_epochs_pretrain}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "torch.save(generator.state_dict(), f\"{model_save_path}/srresnet_pretrained.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n"
      ],
      "metadata": {
        "id": "ILj1-RiJhDQH"
      },
      "id": "ILj1-RiJhDQH",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "eb0cb7ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb0cb7ad",
        "outputId": "6f1b5cf5-187f-44a7-9a9b-2bab31dcb6f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-9b0bcd65678a>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  generator.load_state_dict(torch.load(f\"{model_save_path}/srresnet_pretrained.pth\"))\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:04<00:00, 142MB/s]\n"
          ]
        }
      ],
      "source": [
        "# STEP 7: LOAD PRETRAINED + INIT GAN\n",
        "generator = Generator().cuda()\n",
        "generator.load_state_dict(torch.load(f\"{model_save_path}/srresnet_pretrained.pth\"))\n",
        "discriminator = Discriminator().cuda()\n",
        "vgg_loss = VGGContentLoss().cpu()\n",
        "optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "bce_loss = nn.BCELoss()\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPK8WyQXQI0K"
      },
      "id": "WPK8WyQXQI0K",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "58538c80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "58538c80",
        "outputId": "97634231-a36a-474a-d693-cc1efb8b4781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr_img shape: torch.Size([2, 3, 64, 64]), hr_img shape: torch.Size([2, 3, 256, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-32ba642410d5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Move to GPU before resizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mlr_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Move tensors to GPU first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Resize lr_img to match the size of hr_img (256x256) using interpolate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "#step 8\n",
        "#step 8\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Enable debugging for CUDA kernel errors\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# STEP 8: SRGAN TRAINING\n",
        "for epoch in range(num_epochs_gan):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    for lr_img, hr_img in train_loader:\n",
        "        # Check and print shapes of the images\n",
        "        print(f\"lr_img shape: {lr_img.shape}, hr_img shape: {hr_img.shape}\")\n",
        "\n",
        "        # Ensure tensors are float type\n",
        "        lr_img, hr_img = lr_img.float(), hr_img.float()\n",
        "\n",
        "        # Move to GPU before resizing\n",
        "        lr_img, hr_img = lr_img.cuda(), hr_img.cuda() # Move tensors to GPU first\n",
        "\n",
        "        # Resize lr_img to match the size of hr_img (256x256) using interpolate\n",
        "        import torch.nn.functional as F\n",
        "        lr_img = F.interpolate(lr_img, size=(hr_img.shape[2], hr_img.shape[3]), mode='bicubic', align_corners=False) # Perform resizing on the GPU\n",
        "\n",
        "        # Ensure labels are the correct size and move them to GPU\n",
        "        # real_labels = torch.ones(hr_img.size(0), 1).cuda() # This could lead to shape issues\n",
        "        # fake_labels = torch.zeros(hr_img.size(0), 1).cuda() # This could lead to shape issues\n",
        "\n",
        "        # Discriminator\n",
        "        sr_img = generator(lr_img)\n",
        "        real_out = discriminator(hr_img)\n",
        "        fake_out = discriminator(sr_img.detach())\n",
        "\n",
        "        # Match label shapes and types to the outputs\n",
        "        real_labels = torch.ones_like(real_out) # corrected here\n",
        "        fake_labels = torch.zeros_like(fake_out) # corrected here\n",
        "\n",
        "        d_loss = bce_loss(real_out, real_labels) + bce_loss(fake_out, fake_labels)\n",
        "        optimizer_d.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Generator\n",
        "        fake_out = discriminator(sr_img)\n",
        "        adv_loss = bce_loss(fake_out, real_labels)\n",
        "        # sr_crop = transforms.CenterCrop(hr_img.shape[2:])(sr_img)\n",
        "        import torch.nn.functional as F\n",
        "        sr_crop = F.center_crop(sr_img, hr_img.shape[2:]) # Ensure center crop is done on the GPU\n",
        "        content_loss = vgg_loss(sr_crop.cpu(), hr_img.cpu())\n",
        "        g_loss = content_loss + 1e-3 * adv_loss.cpu()\n",
        "        optimizer_g.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs_gan}] G Loss: {g_loss.item():.4f} | D Loss: {d_loss.item():.4f}\")\n",
        "    torch.save(generator.state_dict(), f\"{model_save_path}/generator_epoch_{epoch+1}.pth\")\n",
        "    torch.save(discriminator.state_dict(), f\"{model_save_path}/discriminator_epoch_{epoch+1}.pth\")\n",
        "\n",
        "torch.save(generator.state_dict(), f\"{model_save_path}/srgan_final.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLGyMf-xQZ4A"
      },
      "id": "fLGyMf-xQZ4A",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5551ee33",
      "metadata": {
        "id": "5551ee33"
      },
      "outputs": [],
      "source": [
        "# STEP 9: VISUALIZE\n",
        "import torchvision.utils as vutils\n",
        "generator.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (lr_img, hr_img) in enumerate(train_loader):\n",
        "        lr_img = lr_img.cuda()\n",
        "        sr_img = generator(lr_img).cpu()\n",
        "        vutils.save_image(sr_img, f\"srgan_output_{i}.png\", normalize=True)\n",
        "        vutils.save_image(hr_img, f\"hr_output_{i}.png\", normalize=True)\n",
        "        vutils.save_image(lr_img, f\"lr_input_{i}.png\", normalize=True)\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60017f46",
      "metadata": {
        "id": "60017f46"
      },
      "outputs": [],
      "source": [
        "# STEP 10: METRICS\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(sr_img, hr_img):\n",
        "    sr = sr_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    hr = hr_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    return psnr(hr, sr), ssim(hr, sr, channel_axis=2)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for lr_img, hr_img in train_loader:\n",
        "        lr_img = lr_img.cuda()\n",
        "        sr_img = generator(lr_img).cpu()\n",
        "        p, s = evaluate(sr_img, hr_img)\n",
        "        print(f\"PSNR: {p:.2f}, SSIM: {s:.4f}\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}